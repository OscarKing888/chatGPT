Python version:  3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]
Command line arguments:
mode: train
dataset: CIFAR10
model: ResNet18
epochs: 100
lr: 0.1
momentum: 0.9
weight_decay: 0.0005
image: ./test
showplot: False


Files already downloaded and verified
Files already downloaded and verified
====Device: cuda:0
Mode: Training, Dataset: CIFAR10, Model: ResNet18, Model file: ResNet18_CIFAR10_best.pth
Epoch 1/100
Train Loss: 2.2025 Acc: 0.2380
Test Loss: 1.6925 Acc: 0.3839

Epoch 2/100
Train Loss: 1.4650 Acc: 0.4566
Test Loss: 1.4242 Acc: 0.4754

Epoch 3/100
Train Loss: 1.2349 Acc: 0.5514
Test Loss: 1.1647 Acc: 0.5855

Epoch 4/100
Train Loss: 1.0557 Acc: 0.6213
Test Loss: 1.1052 Acc: 0.6110

Epoch 5/100
Train Loss: 0.9241 Acc: 0.6713
Test Loss: 1.0034 Acc: 0.6591

Epoch 6/100
Train Loss: 0.8204 Acc: 0.7126
Test Loss: 0.8226 Acc: 0.7091

Epoch 7/100
Train Loss: 0.7347 Acc: 0.7416
Test Loss: 0.8347 Acc: 0.7067

Epoch 8/100
Train Loss: 0.6632 Acc: 0.7706
Test Loss: 0.8004 Acc: 0.7306

Epoch 9/100
Train Loss: 0.6183 Acc: 0.7837
Test Loss: 0.9391 Acc: 0.7007

Epoch 10/100
Train Loss: 0.5730 Acc: 0.8015
Test Loss: 0.7493 Acc: 0.7586

Epoch 11/100
Train Loss: 0.5361 Acc: 0.8138
Test Loss: 0.6909 Acc: 0.7691

Epoch 12/100
Train Loss: 0.5063 Acc: 0.8246
Test Loss: 0.7224 Acc: 0.7638

Epoch 13/100
Train Loss: 0.4847 Acc: 0.8327
Test Loss: 0.5615 Acc: 0.8107

Epoch 14/100
Train Loss: 0.4614 Acc: 0.8411
Test Loss: 0.5736 Acc: 0.8042

Epoch 15/100
Train Loss: 0.4425 Acc: 0.8485
Test Loss: 0.6034 Acc: 0.8004

Epoch 16/100
Train Loss: 0.4188 Acc: 0.8554
Test Loss: 0.6416 Acc: 0.7916

Epoch 17/100
Train Loss: 0.4142 Acc: 0.8570
Test Loss: 0.6338 Acc: 0.7880

Epoch 18/100
Train Loss: 0.3919 Acc: 0.8641
Test Loss: 0.5335 Acc: 0.8180

Epoch 19/100
Train Loss: 0.3827 Acc: 0.8668
Test Loss: 0.5070 Acc: 0.8261

Epoch 20/100
Train Loss: 0.3625 Acc: 0.8750
Test Loss: 0.4837 Acc: 0.8390

Epoch 21/100
Train Loss: 0.3644 Acc: 0.8755
Test Loss: 0.5062 Acc: 0.8327

Epoch 22/100
Train Loss: 0.3517 Acc: 0.8781
Test Loss: 0.5802 Acc: 0.8089

Epoch 23/100
Train Loss: 0.3439 Acc: 0.8814
Test Loss: 0.4813 Acc: 0.8383

Epoch 24/100
Train Loss: 0.3344 Acc: 0.8831
Test Loss: 0.5041 Acc: 0.8300

Epoch 25/100
Train Loss: 0.3326 Acc: 0.8855
Test Loss: 0.5411 Acc: 0.8300

Epoch 26/100
Train Loss: 0.3273 Acc: 0.8867
Test Loss: 0.4934 Acc: 0.8375

Epoch 27/100
Train Loss: 0.3176 Acc: 0.8897
Test Loss: 0.5218 Acc: 0.8314

Epoch 28/100
Train Loss: 0.3215 Acc: 0.8877
Test Loss: 0.5043 Acc: 0.8321

Epoch 29/100
Train Loss: 0.3048 Acc: 0.8939
Test Loss: 0.6519 Acc: 0.7986

Epoch 30/100
Train Loss: 0.3076 Acc: 0.8944
Test Loss: 0.5493 Acc: 0.8216

Epoch 31/100
Train Loss: 0.2978 Acc: 0.8971
Test Loss: 0.6265 Acc: 0.8046

Epoch 32/100
Train Loss: 0.2907 Acc: 0.9001
Test Loss: 0.6461 Acc: 0.7973

Epoch 33/100
Train Loss: 0.2789 Acc: 0.9033
Test Loss: 0.5547 Acc: 0.8281

Epoch 34/100
Train Loss: 0.2843 Acc: 0.9018
Test Loss: 0.7117 Acc: 0.7848

Epoch 35/100
Train Loss: 0.2763 Acc: 0.9043
Test Loss: 0.5101 Acc: 0.8392

Epoch 36/100
Train Loss: 0.2701 Acc: 0.9063
Test Loss: 0.5641 Acc: 0.8206

Epoch 37/100
Train Loss: 0.2741 Acc: 0.9055
Test Loss: 0.5391 Acc: 0.8289

Epoch 38/100
Train Loss: 0.2620 Acc: 0.9079
Test Loss: 0.4738 Acc: 0.8529

Epoch 39/100
Train Loss: 0.2691 Acc: 0.9069
Test Loss: 0.5927 Acc: 0.8147

Epoch 40/100
Train Loss: 0.2570 Acc: 0.9119
Test Loss: 0.4730 Acc: 0.8427

Epoch 41/100
Train Loss: 0.2666 Acc: 0.9087
Test Loss: 0.6133 Acc: 0.8092

Epoch 42/100
Train Loss: 0.2560 Acc: 0.9103
Test Loss: 0.5527 Acc: 0.8279

Epoch 43/100
Train Loss: 0.2569 Acc: 0.9124
Test Loss: 0.6702 Acc: 0.8118

Epoch 44/100
Train Loss: 0.2544 Acc: 0.9124
Test Loss: 0.5098 Acc: 0.8337

Epoch 45/100
Train Loss: 0.2548 Acc: 0.9126
Test Loss: 0.4920 Acc: 0.8481

Epoch 46/100
Train Loss: 0.2350 Acc: 0.9193
Test Loss: 0.4752 Acc: 0.8521

Epoch 47/100
Train Loss: 0.2473 Acc: 0.9136
Test Loss: 0.5177 Acc: 0.8332

Epoch 48/100
Train Loss: 0.2457 Acc: 0.9142
Test Loss: 0.4366 Acc: 0.8594

Epoch 49/100
Train Loss: 0.2407 Acc: 0.9170
Test Loss: 0.4592 Acc: 0.8548

Epoch 50/100
Train Loss: 0.2426 Acc: 0.9164
Test Loss: 0.4423 Acc: 0.8573

Epoch 51/100
Train Loss: 0.1450 Acc: 0.9517
Test Loss: 0.2927 Acc: 0.9092

Epoch 52/100
Train Loss: 0.1054 Acc: 0.9643
Test Loss: 0.2922 Acc: 0.9115

Epoch 53/100
Train Loss: 0.0889 Acc: 0.9703
Test Loss: 0.2956 Acc: 0.9142

Epoch 54/100
Train Loss: 0.0796 Acc: 0.9736
Test Loss: 0.2943 Acc: 0.9136

Epoch 55/100
Train Loss: 0.0720 Acc: 0.9762
Test Loss: 0.2963 Acc: 0.9152

Epoch 56/100
Train Loss: 0.0655 Acc: 0.9787
Test Loss: 0.3015 Acc: 0.9147

Epoch 57/100
Train Loss: 0.0646 Acc: 0.9788
Test Loss: 0.3118 Acc: 0.9127

Epoch 58/100
Train Loss: 0.0573 Acc: 0.9808
Test Loss: 0.3123 Acc: 0.9128

Epoch 59/100
Train Loss: 0.0553 Acc: 0.9818
Test Loss: 0.3147 Acc: 0.9129

Epoch 60/100
Train Loss: 0.0521 Acc: 0.9825
Test Loss: 0.3188 Acc: 0.9138

Epoch 61/100
Train Loss: 0.0485 Acc: 0.9837
Test Loss: 0.3243 Acc: 0.9125

Epoch 62/100
Train Loss: 0.0460 Acc: 0.9849
Test Loss: 0.3173 Acc: 0.9149

Epoch 63/100
Train Loss: 0.0403 Acc: 0.9870
Test Loss: 0.3237 Acc: 0.9140

Epoch 64/100
Train Loss: 0.0397 Acc: 0.9871
Test Loss: 0.3308 Acc: 0.9148

Epoch 65/100
Train Loss: 0.0378 Acc: 0.9879
Test Loss: 0.3337 Acc: 0.9133

Epoch 66/100
Train Loss: 0.0353 Acc: 0.9883
Test Loss: 0.3410 Acc: 0.9122

Epoch 67/100
Train Loss: 0.0357 Acc: 0.9883
Test Loss: 0.3388 Acc: 0.9121

Epoch 68/100
Train Loss: 0.0331 Acc: 0.9889
Test Loss: 0.3465 Acc: 0.9134

Epoch 69/100
Train Loss: 0.0304 Acc: 0.9899
Test Loss: 0.3512 Acc: 0.9133

Epoch 70/100
Train Loss: 0.0300 Acc: 0.9908
Test Loss: 0.3530 Acc: 0.9131

Epoch 71/100
Train Loss: 0.0284 Acc: 0.9911
Test Loss: 0.3615 Acc: 0.9128

Epoch 72/100
Train Loss: 0.0266 Acc: 0.9916
Test Loss: 0.3732 Acc: 0.9108

Epoch 73/100
Train Loss: 0.0261 Acc: 0.9917
Test Loss: 0.3654 Acc: 0.9131

Epoch 74/100
Train Loss: 0.0285 Acc: 0.9904
Test Loss: 0.3664 Acc: 0.9123

Epoch 75/100
Train Loss: 0.0243 Acc: 0.9921
Test Loss: 0.3636 Acc: 0.9126

Epoch 76/100
Train Loss: 0.0223 Acc: 0.9929
Test Loss: 0.3465 Acc: 0.9160

Epoch 77/100
Train Loss: 0.0185 Acc: 0.9943
Test Loss: 0.3485 Acc: 0.9163

Epoch 78/100
Train Loss: 0.0183 Acc: 0.9942
Test Loss: 0.3445 Acc: 0.9166

Epoch 79/100
Train Loss: 0.0167 Acc: 0.9949
Test Loss: 0.3448 Acc: 0.9157

Epoch 80/100
Train Loss: 0.0161 Acc: 0.9950
Test Loss: 0.3455 Acc: 0.9182

Epoch 81/100
Train Loss: 0.0151 Acc: 0.9959
Test Loss: 0.3486 Acc: 0.9173

Epoch 82/100
Train Loss: 0.0150 Acc: 0.9954
Test Loss: 0.3452 Acc: 0.9172

Epoch 83/100
Train Loss: 0.0140 Acc: 0.9962
Test Loss: 0.3485 Acc: 0.9171

Epoch 84/100
Train Loss: 0.0142 Acc: 0.9962
Test Loss: 0.3487 Acc: 0.9180

Epoch 85/100
Train Loss: 0.0140 Acc: 0.9957
Test Loss: 0.3487 Acc: 0.9170

Epoch 86/100
Train Loss: 0.0143 Acc: 0.9956
Test Loss: 0.3500 Acc: 0.9182

Epoch 87/100
Train Loss: 0.0132 Acc: 0.9964
Test Loss: 0.3500 Acc: 0.9179

Epoch 88/100
Train Loss: 0.0135 Acc: 0.9963
Test Loss: 0.3502 Acc: 0.9186

Epoch 89/100
Train Loss: 0.0134 Acc: 0.9961
Test Loss: 0.3493 Acc: 0.9194

Epoch 90/100
Train Loss: 0.0127 Acc: 0.9964
Test Loss: 0.3499 Acc: 0.9186

Epoch 91/100
Train Loss: 0.0132 Acc: 0.9960
Test Loss: 0.3509 Acc: 0.9184

Epoch 92/100
Train Loss: 0.0126 Acc: 0.9967
Test Loss: 0.3543 Acc: 0.9178

Epoch 93/100
Train Loss: 0.0128 Acc: 0.9964
Test Loss: 0.3525 Acc: 0.9177

Epoch 94/100
Train Loss: 0.0125 Acc: 0.9963
Test Loss: 0.3533 Acc: 0.9173

Epoch 95/100
Train Loss: 0.0120 Acc: 0.9968
Test Loss: 0.3534 Acc: 0.9179

Epoch 96/100
Train Loss: 0.0115 Acc: 0.9969
Test Loss: 0.3539 Acc: 0.9172

Epoch 97/100
Train Loss: 0.0107 Acc: 0.9971
Test Loss: 0.3531 Acc: 0.9182

Epoch 98/100
Train Loss: 0.0109 Acc: 0.9968
Test Loss: 0.3564 Acc: 0.9172

Epoch 99/100
Train Loss: 0.0109 Acc: 0.9970
Test Loss: 0.3562 Acc: 0.9176

Epoch 100/100
Train Loss: 0.0111 Acc: 0.9971
Test Loss: 0.3567 Acc: 0.9170

Training complete in 148m 56s
Best val Acc: 0.919400
+-------+----------------------+---------------------+---------------------+---------------------+
| Epoch |      Train Loss      |      Train Acc      |      Test Loss      |       Test ACC      |
+-------+----------------------+---------------------+---------------------+---------------------+
|   1   |  2.2025034154510497  | 0.23796000000000003 |  1.6925125720977783 |        0.3839       |
|   2   |  1.4649712655258178  | 0.45658000000000004 |  1.4241777261734008 | 0.47540000000000004 |
|   3   |  1.234917646331787   |        0.5514       |  1.164709271621704  |        0.5855       |
|   4   |  1.0556671544456482  |       0.62128       |  1.1051535726547241 |        0.611        |
|   5   |  0.9241280109214782  |  0.6712600000000001 |  1.0033513494491577 |        0.6591       |
|   6   |  0.8203759084510803  |  0.7125800000000001 |  0.8226097264289856 |  0.7091000000000001 |
|   7   |  0.7346777442550659  |  0.7416400000000001 |  0.8347115866661072 |        0.7067       |
|   8   |  0.6632460234069825  |  0.7706400000000001 |  0.8004011160850525 |        0.7306       |
|   9   |  0.6182610112190247  |  0.7837200000000001 |  0.9390849752426147 |        0.7007       |
|   10  |  0.5729613083362579  |       0.80152       |  0.7493353613853455 |        0.7586       |
|   11  |  0.5361214965629577  |       0.81376       |  0.6908610273361206 |        0.7691       |
|   12  |  0.5062898760986329  |  0.8246000000000001 |  0.7223981869697571 |        0.7638       |
|   13  |  0.4847310528945923  |  0.8326600000000001 |  0.5615466711997986 |  0.8107000000000001 |
|   14  |  0.4614187315940857  |  0.8411200000000001 |  0.5736434257507325 |        0.8042       |
|   15  |  0.4425042080497742  |       0.84852       |  0.6034123014450073 |        0.8004       |
|   16  | 0.41879257607460024  |  0.8554200000000001 |  0.6415879286766052 |  0.7916000000000001 |
|   17  | 0.41418344827651976  |  0.8570000000000001 |  0.6337597629547119 |        0.788        |
|   18  |  0.391949187707901   |       0.86414       |  0.5335040985107422 |  0.8180000000000001 |
|   19  | 0.38270611142158506  |       0.86682       |  0.5070498170852661 |  0.8261000000000001 |
|   20  |  0.3624576109981537  |  0.8750000000000001 | 0.48365780029296873 |  0.8390000000000001 |
|   21  | 0.36440619364738464  |  0.8755000000000001 |  0.5061844694137573 |        0.8327       |
|   22  |  0.3517263144493103  |  0.8781000000000001 |  0.5802182570457458 |  0.8089000000000001 |
|   23  | 0.34387519159317015  |  0.8814200000000001 |  0.4813158479690552 |        0.8383       |
|   24  | 0.33439470635414126  |  0.8831200000000001 |  0.504065547657013  |  0.8300000000000001 |
|   25  |  0.332590476360321   |  0.8855400000000001 |  0.5410702960968018 |  0.8300000000000001 |
|   26  | 0.32732245516777037  |  0.8867400000000001 | 0.49338006162643433 |        0.8375       |
|   27  |  0.317639754781723   |  0.8897200000000001 |  0.5218341582775116 |        0.8314       |
|   28  |  0.3214744012260437  |  0.8877400000000001 |  0.5043136869430542 |  0.8321000000000001 |
|   29  |  0.3047646441936493  |        0.8939       |  0.6518993041992187 |  0.7986000000000001 |
|   30  |  0.3075535648918152  |  0.8944000000000001 |  0.5493119338035584 |        0.8216       |
|   31  |  0.2978226853275299  |  0.8970600000000001 |  0.6265056303977966 |  0.8046000000000001 |
|   32  |  0.2906777064704895  |  0.9000800000000001 |  0.6460596871376038 |        0.7973       |
|   33  | 0.27886685293197633  |  0.9032600000000001 |   0.55468534450531  |  0.8281000000000001 |
|   34  | 0.28430845497131346  |  0.9018200000000001 |  0.7116902956962585 |        0.7848       |
|   35  | 0.27631363085746763  |  0.9042800000000001 |  0.5100778025627136 |  0.8392000000000001 |
|   36  | 0.27014972710609436  |  0.9063200000000001 |  0.5641001480102539 |        0.8206       |
|   37  | 0.27414650049209593  |  0.9055000000000001 |  0.5390655941009521 |  0.8289000000000001 |
|   38  |  0.2620260364580154  |  0.9078600000000001 | 0.47377006101608277 |        0.8529       |
|   39  | 0.26908917410850525  |        0.9069       |  0.5927349854469299 |  0.8147000000000001 |
|   40  | 0.25704806645393374  |        0.9119       | 0.47297842025756837 |        0.8427       |
|   41  |  0.2665576537752152  |       0.90866       |  0.6132727924346923 |        0.8092       |
|   42  | 0.25600057745933535  |       0.91034       |  0.5527487183570862 |  0.8279000000000001 |
|   43  |  0.256891186876297   |  0.9123800000000001 |  0.6702493838310242 |  0.8118000000000001 |
|   44  |  0.2543700171661377  |  0.9124000000000001 |  0.5098128597736359 |        0.8337       |
|   45  |  0.2548346781158447  |  0.9126000000000001 |  0.4920086091041565 |  0.8481000000000001 |
|   46  |  0.2349600118970871  |       0.91932       | 0.47520746755599974 |  0.8521000000000001 |
|   47  | 0.24726717420101166  |  0.9135800000000001 |  0.5177142289161683 |        0.8332       |
|   48  | 0.24569740481853486  |       0.91424       | 0.43661266250610353 |        0.8594       |
|   49  | 0.24068443460464478  |  0.9169600000000001 | 0.45915276975631714 |        0.8548       |
|   50  |  0.242584345703125   |       0.91644       |  0.4422627048492432 |  0.8573000000000001 |
|   51  | 0.14496966922044754  |       0.95174       |  0.2926902370929718 |        0.9092       |
|   52  | 0.10536661152601243  |  0.9642600000000001 | 0.29217380628585815 |  0.9115000000000001 |
|   53  | 0.08888611305475234  |       0.97028       | 0.29558108806610106 |        0.9142       |
|   54  | 0.07962673115611077  |  0.9735600000000001 | 0.29430171389579773 |  0.9136000000000001 |
|   55  | 0.07195576453208924  |  0.9762000000000001 | 0.29628950052261355 |        0.9152       |
|   56  | 0.06548372680187226  |  0.9786800000000001 | 0.30149520421028136 |  0.9147000000000001 |
|   57  | 0.06463706723690033  |  0.9787800000000001 |  0.311754660320282  |  0.9127000000000001 |
|   58  | 0.05726921891927719  |  0.9807800000000001 | 0.31234675507545473 |  0.9128000000000001 |
|   59  |  0.0552976097536087  |  0.9818000000000001 |  0.3147251196861267 |        0.9129       |
|   60  | 0.05213947237968445  |  0.9824800000000001 | 0.31879646208286283 |  0.9138000000000001 |
|   61  | 0.04845909714221954  |  0.9837400000000001 |  0.3243250878334045 |  0.9125000000000001 |
|   62  |  0.045996828186512   |       0.98494       | 0.31731259303092957 |        0.9149       |
|   63  | 0.040308405060768125 |  0.9870000000000001 | 0.32365334486961367 |        0.914        |
|   64  | 0.03965292285323143  |  0.9871400000000001 | 0.33076360874176025 |  0.9148000000000001 |
|   65  | 0.03775448925256729  |  0.9879000000000001 | 0.33374504070281985 |        0.9133       |
|   66  | 0.03531741337776184  |       0.98828       | 0.34097935953140257 |        0.9122       |
|   67  | 0.03565388657152653  |       0.98826       | 0.33884863209724425 |        0.9121       |
|   68  | 0.03305575351953507  |  0.9889200000000001 |  0.346514232635498  |        0.9134       |
|   69  | 0.030352681480646133 |  0.9899000000000001 | 0.35115842332839964 |        0.9133       |
|   70  | 0.029951139602065087 |       0.99082       |  0.3529972760677338 |        0.9131       |
|   71  | 0.02835229615211487  |  0.9910800000000001 | 0.36153148698806764 |  0.9128000000000001 |
|   72  | 0.026562440323233603 |        0.9916       |  0.373215771150589  |        0.9108       |
|   73  | 0.026109638092517852 |  0.9916600000000001 |  0.3653608410358429 |        0.9131       |
|   74  | 0.028504756619930266 |       0.99038       |  0.366426108455658  |        0.9123       |
|   75  | 0.024336514486670496 |       0.99206       | 0.36359181370735166 |  0.9126000000000001 |
|   76  | 0.022310978498458863 |  0.9929000000000001 |  0.3464689739704132 |        0.916        |
|   77  | 0.01852311023056507  |  0.9943000000000001 | 0.34850788617134093 |        0.9163       |
|   78  |  0.018298936201334   |  0.9942400000000001 | 0.34445435128211976 |  0.9166000000000001 |
|   79  | 0.01668959205240011  |  0.9948600000000001 | 0.34475336651802063 |  0.9157000000000001 |
|   80  | 0.016074082755744457 |  0.9950000000000001 | 0.34551717896461487 |        0.9182       |
|   81  | 0.015092579476237298 |  0.9959000000000001 |  0.3486164197444916 |        0.9173       |
|   82  | 0.015034041538834572 |  0.9953600000000001 | 0.34524092268943785 |        0.9172       |
|   83  | 0.013990143414735794 |  0.9962400000000001 | 0.34851107444763185 |        0.9171       |
|   84  | 0.01415133072733879  |  0.9961800000000001 |  0.3487437511920929 |        0.918        |
|   85  | 0.013998309441208839 |  0.9956600000000001 |  0.3486845776081085 |        0.917        |
|   86  | 0.014319233691990375 |  0.9956400000000001 | 0.35004355721473696 |        0.9182       |
|   87  | 0.013197061569690704 |  0.9963600000000001 |  0.3499866727352142 |        0.9179       |
|   88  | 0.01345858966305852  |  0.9963200000000001 |  0.3502059304714203 |  0.9186000000000001 |
|   89  | 0.013409028893113135 |  0.9961000000000001 |  0.3492960726261139 |        0.9194       |
|   90  | 0.012684441696703434 |       0.99638       |  0.3498834716796875 |  0.9186000000000001 |
|   91  | 0.013198747053742408 |  0.9959600000000001 | 0.35087901248931885 |        0.9184       |
|   92  | 0.012624751631021499 |       0.99672       |  0.3542584195137024 |  0.9178000000000001 |
|   93  | 0.012822842140197754 |  0.9963600000000001 |  0.3525236953258514 |  0.9177000000000001 |
|   94  | 0.012523934811353683 |  0.9963000000000001 |  0.3533166656970978 |        0.9173       |
|   95  | 0.011970997031927109 |  0.9968000000000001 | 0.35344474387168884 |        0.9179       |
|   96  | 0.011494537354409695 |  0.9968800000000001 |  0.3538698787212372 |        0.9172       |
|   97  | 0.010668849349617959 |  0.9971000000000001 | 0.35305100440979004 |        0.9182       |
|   98  | 0.01088630030810833  |  0.9968000000000001 | 0.35641744928359986 |        0.9172       |
|   99  | 0.01092773341625929  |       0.99704       | 0.35623478946685794 |  0.9176000000000001 |
|  100  | 0.011092508331537246 |  0.9970600000000001 |  0.3566958261013031 |        0.917        |
+-------+----------------------+---------------------+---------------------+---------------------+
Time elapsed: 8939.13 seconds
